# Kaggle_comp
Дневник своих действий и сохраненные решения с скором

__День 1__:
----
Прочитан [ресурс](https://habr.com/ru/company/ods/blog/426227/) на habr про правильный подход к
решению задачи:
- Нужно заострить внимание на __валидации__, один из возможных ресурсов для машнного обучения на
  выборке: `KFold` из модуля `sklearn`. Разбть выборку на тренировку и тест, затем обучать и
  тестировать. Но как преобразовать потом значения для целевого признака y?
- Коммитить каждый результат со скором, чтобы безболезненно откатиться к стабильному решению
- Просматривать кернелы, но до них еще 2 недели.
- Создать Pipe со всеми методами, который потом модернизировать

Наблюдения:
- Распределения x_*_0 похоже на равномемрное R[0, 3],
- Распределения x_*_1 похоже на нормальное с большим коэффициентм эксцесса,
- Распределения x_*_2 похоже на синус(???), на каждой из x_1_2, x_2_2, x_3_2, ... скачки через
  определенное число элементов
- Распределение целевого признака похоже на нормальное смещенное распределение

Идеи решения:
- Построить нейронную сеть с скрытыми слоями и после кросс-валидации запустить процесс
  обучения(риск: долгая обучение, переобучение)

Решение:
- огромная ошибка, классификация RandomForest не сработала для этой задачи
- огромная ошибка 2, классификация нормального распределения не сработала

__День 2__:
---

Идеи решения:
- Так как задача регрессии, нужно:
    1. Выделить информативные признаки из всех
    2. Через методы регрессии получить их значение
- После этих действий результатом будет скалярное произведение всех 300 массивов-признаков
  размерности 10000(матрица $A \in \R^{10000\times300}$) на полученный в результате регресии вектор 
  $b\in\R^{300}$. Итогом работы программы будет вектор $y = A\times b; y \in \R^{10000}$

Отправки:
- После применения линейной регрессии к данны ошибка достигла 30.
- Проверка метода корреляций -- ошибка 30, нет сильно значимых признаков, так как максимум
  корелляции был 0.11. Возможно есть незначимые признаки, так как минимум -0.02. В ходе проверки
  __не найдены__ признаки, корреляция которых равна 0.

Написана кросс-валидация с параметром n_splits = 10(эксперементально проверенное
значение/советы из литературы), поэтому выборка train разбита на тренировочную и тестовую.


Реализован ансамбль моделей из модуля sklearn. Для каждой модели подсчитывается точность, потом
лучшие варианты своих моделей(например, для моделей KNN и
RandomForestClassifier). 

- В ходе выполнения обучения выявлено, что для KNN оптимальным
значением соседей будет 23.
- В ходе выполнения обучения для модели RandomForest оптимальным значением n_estimators из
  выборки(50, 100, 200) оказалось 100
- Проведено обучение моделей SGDClassifier, LogisticRegression и показаны значения точностей
  этих моделей:
```bash
KNN 0.053
RandomForest 0.072
Logistic 0.046
SGD 0.038
Gaussian 0.051
Ada 0.065
DecTree 0.04

ensemble 0.046(лучший результат 0.057)
```

Лучше всех сработал алгоритм SGD. После проверки выяснилось, что ошибка на тестовых данных
меньше предыдущих, а также в несколько раз меньше ошибок прошлых посылок. После загрузки в kaggle
результат: 6.68..., лучше методов, связанных с корреляцией и регрессиией.

__Подход к задаче с точки зрения теории вероятностей__

Возможно, стоит рассмотреть нормальное распределение с мат.ожиданием 18.3, $\sigma = 6.87$, так
как график распределения похож на распределение y.

Полученный график гистограммы y очень похож на нормальное распределение с указанными
параметрами.

__День 3__
---
Идеи:
- Так как SVG модель лучше справилась с данными, проверить ее на всей выборке(Результат:
  ошибка уменьшилась, но незначительно -- та же ошибка 6)
- Реализовать правильный подсчет корреляции Пирсона для признаков, проверить явные нули, затем
  проверить значимость корреляций через t-критерий Стьюдента для того, чтобы убрать незначимые
  признаки

Из-за невнимательности рассматривалась модель с худшим параметром точности(который
принимался за ошибку), а самая точная модель в реализации кросс-валидации оказалась
Random-forest, поэтому проверена точность модели при различных параметрах n_estimators:
- n_estimators = 10000 => score = 5.36678 
- n_estimators = 1000 => score = 5.36579
- n_estimators = 500 => score = 5.36579

__Очень интересное наблюдение__
Построен массив корреляций признаков с результатом. Для этого массива посчитан t-критерий Стьюдента
и построен доверительный интервал для 95% -- то есть 95%, что случайная величина(t-критерий
Стьюдента корреляции вектора $y$ с признаком $X_i$)
попадет в искомый интервал. Далее для всех t-критериев проводится отбор:
- если t-критерий корреляции $y$ с признаком $X_i$ не входит в доверительный интервал, то
  маловероятно, что оцененная корреляция:
    - будет входить в 95%-ую вероятность(то есть $y$ практически не зависит от этого признака)
    - вовсе не является корреляцией(то есть скрытые признаки выдали сходство, но сам $y$ от
      конкретного признака не зависит)

По результатом отбора получено число признаков, которые прошли отбор и их индексы:
```bash
32
[59, 62, 71, 83, 101, 107, 110, 119, 140, 142, 152, 162, 168, 174, 189, 190, 198, 205, 211, 222,
226, 228, 232, 240, 241, 246, 252, 259, 261, 276, 291, 298]
```

Лишь $\frac{32}{300}$ признаков оказались значимыми.

__Объединение статистических выводов и лучшего регрессора__

Матрица признаков заменена на матрицу значимых признаков, затем по ней проводится оценка
лучшего регрессора из sklearn:
```bash
KNN 0.038608519097921956
RandomForest 0.04170718524912087
Logistic 0.069
SGD -9.574491199245156e+22
Gaussian 0.052
Ada 0.020337533027445565
DecTree 0.036
```

Результатом ансамбля лучших моделей оказался результат: -0.2347772420088041, поэтому используем
для тестирования модель LogisticRegression для тех 32 признаков, которые показались
значимыми. Score = 5.96720.

Если проверить данный алгоритм на модели RandomForest(n_estimators = 1000) для 32-ух данных, то
получится результат 5.43165, который отличается от лучшего на текущий момент такого же
регрессора, но для 300 признаков с score = 5.36579 не очень значительно. То есть сокращение
выборки в 10 раз выявило значимые результаты.

__День 4__
----
Реализована модель метода наиментших квадратов(линейная), применена для 32-х признаков -- score =
5.4

Увеличено доверительный интервал до показателья 99%, теперь значимых 42 признака, для них
применена модель метода наименьших квадратов -- score = 5.22 -- пока что лучший результат

Проверено, что если уменьшить доверительный интервал до 90%, то для того же МНК score увеличится,
поэтому лучше работать с 42-мя признаками.

Для удобства добавлена функция измерения ошибки -- __средняя абсолютная ошибка__

__Итог__: модель лучше работает для 42-х значимых признаков, получаемых в результате подсчета
доверительного интервала для параметра 99%

Нужно попробовать подобрать параметры для лучшей модели регрессии(в ходе оценки ей оказалась модель
RandomForest) -- для понимания значимости параметров ознакомился с 
[материалом](https://dyakonov.org/2016/11/14/%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D0%B9-%D0%BB%D0%B5%D1%81-random-forest/comment-page-1/):
  - Лучшим значением параметра `n_estimators` оказалось значение __5000__
  - Лучшим значением параметра `max_features` оказалось значение (как и написано в материале, так и
    на практике: при больших значениях точность выше, но процесс дольше, поэтому средним выбрано
    пока что значение 5)

  
Проверен метод наименьших квадратов для полинома 3-ей степени, результат лучше: 
- score = 5.04

Выявлены 42 значимых признака, но возможно, что их корреляция не так значима, потому что могут
быть скрытые зависимости от 3-их признаков, поэтому создан алгоритм глубокого обучения для
выявления скрытых признаков. Для глубокого обучения с одним скрытым слоем размерности 128
значение ошибки было 6.39, на тренировочной выборке средняя абсолютная ошибка достигла 6.68, на
новой архитектуре: два скрытых слоя, первый размерности 200, второй - 100 нейронная сеть уже
получает меньшую ошибку. Возможно, результат придется загрузить завтра, так как долгое обучение.

__День 5__
----

- Созданы архитектуры глубокой сети:
    - 1 слой размера 128(score = 6.39)
    - 2 слоя размеров 200 и 100(score = 5.47)
    - 3 слоя размера 200, 100 и 50(score = 7.6..)

Плохой результат произошел из-за недообучения модели или переобучения. Лучшей моделью является двуслойная(200, 100), но нужно ее дообучить.

- Подбор параметров для RandomForestResgressor не осуществлен, так как значительная часть времени была потрачена на разбор работы jupyter notebook на GPU. Иначе модель долго тестировалась, а с GPU возникли проблемы, поэтому нужно доразбраться в алгоритме CUDA и ускорить подбор параметров.

__День 6__
----

Проверка точности метода наименьших квадратов:
- Заранее плохая затея: искать методом нименьших квадратов зависимость в виде полином 12-ой
степени. После 3-х часов поиска ошибка была оочень большой, порядка $10^{12}$
- После сокращения степени полинома до 5-ти, поиск составил примерно минуту, ошибка в смысле `mae`
  составила 4.901214299536031.
- Повышение степени полинома до 6-ти привело к уменьшению ошибки до значения 4.870604365694106
- Повышение степени до значения 7 приводит к ошибке:

```bash
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-53-f031cd6d637d> in <module>
----> 1 cofs, _ = curve_fit(f, Matrix1, y)

~/.local/lib/python3.8/site-packages/scipy/optimize/minpack.py in curve_fit(f, xdata, ydata, p0, sigma, absolute_sigma, check_finite, bounds, method, jac, **kwargs)
    787         cost = np.sum(infodict['fvec'] ** 2)
    788         if ier not in [1, 2, 3, 4]:
--> 789             raise RuntimeError("Optimal parameters not found: " + errmsg)
    790     else:
    791         # Rename maxfev (leastsq) to max_nfev (least_squares), if specified.

RuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 59200.
```
Поэтому наибольшей степенью полинома для оценки методом наименьших квадратов является 6. Для МНК
с полиномом 6-ой степени score = 4.97575 (пока что является лучшим результатом моих отправок).

Теперь пробую провести приближение для всей выборки, а не только для 42-х значимых признаков. При
всей выборке полином 300-ой степени состоит из 1801 переменной, и подбор происходит уже 6 часов,
поэтому принято решение увеличить доверительный интервал до показателя 99.999%, при котором значимыми
показываются 64 признака.

Время выполнения составило примерно 10 минут. score = 4.7777(лучший результат).

Прочитан [ресурс](https://habr.com/ru/company/mailru/blog/513842/) про глубокое обучение в регрессии и другие популярные модели регрессоров.
Замечено, что практически все использованы для проверки или не подходят для данной
задачи(например, модель гребневой регрессии не подходит для данной задачи, так как расчитана на
увеличение кол-ва "значимых" признаков), но нужно протестировать еще регрессоры:
- регрессиия LASSO
- регрессия ElasticNet с параметром $\alpha$ близком к 1, так как при $\alpha = 0$ регрессия
  становится гребневой.

Для применения глубокого обучения использовались доп слои с нелинейными функциями
активации(sigmoid), архитектура состояла из трех слоев: 100, 50, 25, и между ними слои с
функциями активации sigmoid. Обучение долгое, но на каждом этапе сохраняется текущее состояние и
выводится информация об ошибке на выборке, относенной к категории test после применения
кросс-валидайии(то есть будет обнаружено переобучение):
```bash
Средняя абсолютная ошибка: 5.203114032745361 (100 fits)
Средняя абсолютная ошибка: 4.841518402099609 (200 fits)
Средняя абсолютная ошибка: 5.086235046386719 (300 fits)
Средняя абсолютная ошибка: 4.839902877807617 (400 fits)
Средняя абсолютная ошибка: 4.78037166595459 (500 fits)
Средняя абсолютная ошибка: 4.90826940536499 (700 fits)
Средняя абсолютная ошибка: 4.696959018707275 (800 fits)
Средняя абсолютная ошибка: 4.737911224365234 (1000 fits)
Средняя абсолютная ошибка: 4.87761116027832 (1500 fits)
Средняя абсолютная ошибка: 4.773355484008789 (2500 fits)
```

Но модель обучалась на 300 признаках, тем не менее score = 4.84(среди алгоритмов машинного обучения
пока что лучший результат)


Закончен подбор оптимальных параметров для RandomForestRegressor. Параметры:
```bash
{'max_depth': 5, 'max_features': 42, 'n_estimators': 5000, 'n_jobs': -1}
```

Для 64-х признаков результат работы RandomForest составил score = 5.36

__Итоги__:
- Deep learning 3 слоя -- 100, 50, 25 -- 2500 fits -- 300 признаков -- score = 4.84 --
  замечание(взять только значимые признаки, так как выявляются засимости между всеми признаками, в
  том числе и между "шумом")
- RandomForest -- подобраны параметры -- 64 признака -- score = 5.36 -- замечание(max_features =
  число признаков) -- для 300 признаков
- МНК -- зависимость до 6-ой степени -- 64 признака -- score = 4.77 -- замечание(понизить степень и
  сделать для 300 признаков)

__Сделать__:
- LASSO и ElasticNet

__Использован метод МНК кубической зависимости для ВСЕХ признаков и результат оказался score = 3.43 (теперь лучший
результат)__

__День 7__
----
Стали известны имена признаков.

Подробнее рассмотрены типы значений данных:
- в типе операции значения только {0, 1, 2, 3} - 4 операции
- в первом параметре только целые характеристики(от 0 до 100) - предположение: кол-во вещества/
  концентрация(округленная до целого числа)/номер в таблице
- во втором параметре дробные значения от 0 до 3: кол-во дней, деленное на 10(один месяц)

__В ходе проверки гипотез установлено__, что операции с типами 1 и 3 обладают двумя характеристиками(без
-1 среди этих значений), у операций с типом 2 характеристика №1 принимает значение -1, у
операции с типом 0 характеристика №2 принимает значение -1, то есть:
- типы операций 3 и 1 свойсвтенны бинарным операциям(участвуют две характеристики)
- тип операции 2 свойственен унарной операции с 2-ой характеристикой(без участия первой)
- тип операции 0 свойственен унарной операции с 1-ой характеристикой(без участия второй)

Неудачная попытка попробовать нормолизовать 1-ую характеристику, разделив ее на 100, привела к score
= 4.18.

Использование лучшей до знания имен признаков модели(МНК 3-ей степени) к подготовленной к
работе матрице(то есть с замененными минус единицами на ноль) привело к улучшению результата
на 0.00002, теперь score = 3.43255.


__День 8__
---

Гипотеза:
- 1-ая характеристика - концентрация в процентах(например, соли в воде)
- 2-ая характеристика - масса смеси
- типы операций:
    - 0 - операция достижения заданной концентрации
    - 1 - операция добавления смеси заданной массы к имеющемуся раствору
    - 2 - операция добавления воды(нейтральный элемент) к раствору
    - 3 - обратная к 1 операция
- ПРОБЛЕМЫ:
    - в столбце первого признака(операции) наблюдаются как операции добавления, так и операции убавления вещества.
        - Решение: изначально имеется некоторая масса вещества(но как определить?)

Смоделирована модель, основная проблема, что если иметь значения массы соли и массы воды, то начиная с 10-ой(примерно) операции большинство испытаний показывают, что масса соли становится отрицательной. Причины:
- Малая начальная затравка(взять большое значение массы)
- Неверная гипотеза

Чтобы проверить корректность гипотезы рассмотрено грубое приближение(смещенное на мат.ожидание y в положительном направлении, то есть результат += 18.3): в зависимости от операции(1 или 3) прибавлять или убавлять произведение X_i_1*X_i_2/100. 

Тогда для случая, когда 3 - операция '+', 1 - операция '-', ошибка составила 8.94708953, график(зеленая гистограмма - смоделированное значение): 

![](https://psv4.userapi.com/c536132/u140757976/docs/d9/8fda0fd252e2/3_1.png?extra=VKqduhmTLNpg2YSE252exTH46-gqiCFPmbXSBXkR0Kc4utEpAqgUYnn5uRsOJbLHfuMFcPKSGwWpaYYWjphpmo1mnZOooF9H6xwOGuV4mVP_Ek8hU89b6AQviW_Gs0hnC6ajUu4fqq8pED_nl_pZmYc)

Тогда для случая, когда 3 - операция '-', 1 - операция '+', ошибка составила 6.98379433, график(зеленая гистограмма - смоделированное значение): 

![](https://psv4.userapi.com/c536132/u140757976/docs/d15/7ecdca89a725/3-1.png?extra=JXPArC-TUmHG_4pcOh9Hdx-Z3TIV8Rt-yH3j0ma75of5UPr2gCkqTXMz3M088y7rpRByRxcirUiXmH6MmIV3bjBkgjmeUTabTHbtu7RNBHCvyelEDQ5yEeUxgrUjw0hFoDINXpVWFRMdEt-GBrFEE3o)

__День 9__
----

Улучшена предыдущая гипотеза.

__ОПЫТ__: Изучение зависимости растоворимости от температуры

__МАТЕРИАЛ__:
- вода($H_2O$)
- соль(NaCl)

__ДАННЫЕ__:
- 2 характеристики каждой операции:
  - Температура вещества (целое число от 0 до 100 градусов Цельсия)
  - масса(вещественное значение от 0 до 3) в граммах
- 4 операции:
  - 0 - нагрев смеси до заданной температуры
  - 1 - добавление соли заданной температуы заданной массы в раствор
  - 2 - извлечение выпавшего осадка соли
  - 3 - добавление воды заданной температуры заданной массы в раствор
- __РЕЗУЛЬТАТ__ - значение концентрации

__Почему такая гипотеза__: 
- Изменение характеристик, непосредственно оказывающих влияние на растворимость(температура,
  концентрация, включающая массу воды и массу соли)
- Реальность операций: логично их выполнение(нет извлечения заданной концентрации из смеси, как в
  предыдущей гипотезе)
  
__Проблемы__:
- Так как среди операций есть извлечение осадка соли, и эта операция может оказаться первой в
  экспериментах, то нужны изначальные данные: масса воды и масса соли. Как их узнать?
  
__Потенциальное решение__:
- Запустить цикл для отбора, в котором принимаютя различные значения массы воды, массы соли, чтобы
  выполялось два условия:
  - Отсутствие отрицательной массы соли в любой момент эксперимента
  - Минимальность ошибки смоделированной величины и известной

Результат корректности гипотезы запланирован на завтра.

__День 13__
---

Дополнительные проблемы при реализации:
- как и упоминалось выше, частое возникновение отрицательной массы соли. __Решение__: найдено 
наибольшее значение "забора" соли в опыте(строке матрицы), таким значением оказалось число 74.09. 
Пологается взять массу соли равной 75.
- В ходе проверки гипотезы выяснилось(интернет, результаты ноутбука), что значение концентрации соли в 
растворе не показывает значимой зависимости от температуры, тогда получается, что операции 1 и 3 - "бесполезные".
Поэтому рассмотрен другой, смежный опыт, в котором нужно определить значение температуры после преобразований. 
Для подбора затравочных значений выбран фиксированный параметр соли, затем циклом пробегаются значения массы воды, 
для меньшей ошибки. После этого происходит подбор 3-го параметра.

Подбор параметров температуры не привел к ошибке меньше, чем 14,5.

__День 14__
---

Улучшено значение минимального начального значения массы соли. Вместо простого выбора наибольшей
суммы изымаемой массы соли каждый раз при встрече метода 1, когда соль добавляется в
раствор, уменьшается искомая сумма. Получено, что новое минимальное значение массы составляет
57.61 $\approx$ 58. Проверено, что при значениях $\le$ 57 появляются отрицательные массы
солей в ходе экспериментов.

Замечено, что из-за операции установления заданной температуры(операция типа 0) зависимость ответа
от исходных данных почти не заметна, поэтому нужно сформировать новую гипотезу для этой операции.
Сначала это операция исключается из рассмотрения, проверяются ошибки на полученной модели.
При тривиальных значениях начальных параметров(температура = 0, масса воды = 100г, масса соли =
58г) получается ошибка 5.8795, то есть несмещенная величина дает погрешность $\sim$ 6, результат
кажется лучше предыдущих реализаций гипотез, поэтому производится подбор параметров.

Фиксирование параметра и подбор более оптимальных остальных не поможет найти лучшие значения, так
как при изменении температуры значительно меняется лучшее значение массы воды, а при изменении массы
воды значительно меняется температура. Поэтому подбор просиходит сразу всех параметров:
__значения не подобраны, так как алгоритм не завершил их подсчет, стала известна верная
гипотеза(про перемещение механической ленты и смещивание руды)__.

__Стали известны операции и смысл целевой переменной__
---

Эксперименты: перемещение руды по ленте

Материалы: руда и механическая лента

Типы операций:
- включается режим глобального перемешивания(лента трясётся и руда перемешивается )
- включается режим локального перемешивания(в определённом месте ленты специальными "миксерами" руду перемешивают )
- включают движение (сама лента движется - с её конца продукция ссыпается в хранилище)
- помещение руды на ленту

__Целевой признак__ - хим. состав руды, которая после последней операции находится на ленте.

__День 15__
---

Стали известны коды операций:
- 0 = сдвиг ленты (продолжительность сдвига, -1)
- 1 = добавление руды (в какую часть ленты, какая марка руды) -- одинаковое колличество, но неизвестное  
- 2 = глобальное перемешивание (-1, интенсивность)
- 3 = локальное перемешивание (в какой части ленты, интенсивность)

Проверил, что возможные марки руд - это 0, 1, 2 или 3.

__Гипотеза__

После попадания в сегмент ленты всех 4-х марок руд и после их перемешивания образуется нужная
смесь. __Задача__: посчитать сколько сегментов с необходимой смесью на ленте останется после
последних 100 операций.

- Если происходит перемешивание по всей механической ленте, то все сегменты, в которых содеражались
  4 элемента образуют нужную смесь
- Если происходит локальное перемешивание, то при наличии всех 4-х нужных элементов образуется
    нужная смесь.
- Если происходит сдвиг, то всё содержимое сегментов, попавших в число сдвигаемых,
  обнуляется.
  
__Результат__: гипотеза неверна, так как почти в каждой строке таких сегментов 0, кроме строк 4024,
5081, 8245, где значения составили 1.

Для случая, когда в сегменте хотя бы одна руда после пробразований, результат составил: 
- среднее число таких сегментов равно 12.0674
- отличие от целевой переменной 7.0396

__День 16__
---

Созданы методы для каждой операции для более удобного построения модели.

Гипотеза: считаем число ячеек, где образуется нужная смесь.
- Нужная смесь образуется после перемешивания в ячейке, содержащей все 4 элемента
- При равномерном распределении считается, что везде появляются все 4 элемента(то есть в каждой
  ячейке есть какое-то их кол-во)
- Сдвиг ячейки сбрасывает первые сегменты, а последние зануляет

__Результат__: мало искомых ячеек(в среднем число составило 12), нерегулируемый параметр, потому что
- Оценка движений ленты показала, что в среднем на 1 добавленную руду приходится 10 сброшенных
  сегментов, поэтому иало искомых сегментов и нельзя регулировать состояние(любое начальное
  состояние очень быстро перейдет в ноль).
  
Оказалось, что размер ленты фиксированный, то есть внешние параметры, например, приход заполненного
до верху 105-го сегмента невозможен.

Гипотеза: все, как и описано выше, но с массой(масса неизвестна, но одинакова, поэтому можно
пологать ее равную 1).
- Пологается, что сдвиг происходит так:
    - Первые(или же последние) сегменты обнуляются, а остальные не трогаются(сдвинулось для сброса и
      вернулось назад)
- Образование новой смеси происходит как и в гипотезе выше, но параметр масса, поэтому масса новой
  смеси - это 4*минимальная масса среди 4-х руд(то есть пересечение)
- Глобальное перемешивание равномерно распределяет массы руд по ленте
- Смесь - это не 5-ый элемент, а скорее дополнительная характеристика(как таковой этой смеси нет, но
  ее масса все время хранится), поэтому после глобального перемешивания и происходит перерасчет этой
  смеси.
- Результирующая величина - это концентрация этой "смеси".

Установлено, что такая гипотеза приводит к примерно равномерному распределению концентраций около
параметра 0.25(25%).

Сформирована гипотеза о смысле интенсивности: интенсивность показывает, какую часть от общей
равномерно распределенной суммы масс руд мы добавляем/удаляем из сегментов. Интенсивность локального
перемешивания показывает, какая часть от пересечения масс попадает в результирующую смесь. Так как
интенсивность не больше 3-х, нужно нормировать ее на это значение. Гипотеза будет проверена щавтра.

__День 17__
---

Реализована модель с учетом интенсивности глобального перемешивания. Пусть на 101 сегменте ленты расположены $m_i$
массы руд. Не ограничивая общности, будем рассматривать руду только первой марки, так как в
гипотезе о глобальном перемешивании предполагается стремление к равномерному распределению масс
по сегментам. Тогда равномерно распределенное значение массы на каждом сегмента составляло бы
$a = \frac{\sum_{i=0}^{100}m_i}{101}$. Тогда учитываем нормализованную интенсивность
$intens = \frac{intensivity}{3}$(в ходе
проверки установлено, что наибольшее значение интеснсивности состаяляет 3.0), на данную величину
домножаем полученное ранее значение массы. Теперь к каждому сегменту применим соответствующую
формулу: $\overline{m_i} = m_i*(1-intens) + a*intens$, тогда
$\sum_{i=0}^{100}\overline{m_i} =
\sum_{i=0}^{100}m_i-intens*\sum_{i=0}^{100}m_i+101*a*intens=\sum_{i=0}^{100}m_i$,
то есть общая масса сохраняется постоянной. 

Для локального перемешивания гипотеза схожа: Пусть в рассматриваемом сегменте содержится $m_1$
массы 1-ой руды, $m_2$ - 2-ой, $m_3$ - 3-ей, $m_4$ - 4-ой, тогда нужное вещество(целевое)
получается из описанных масс, как $4*min(m_1, m_2, m_3, m_4)$, причем нормализованная
интенсивность показывает, какая часть от этой массы действительно получится(некоего
рода выход реакции).

Модель реализована, но неизвестными остаются начальные параметры и добавляемая масса. В
предположении, что начальные значения распределены равномерно в диапозоне $[0, 1]$, а масса при
добавлении руды составляет 1, получаем, что ошибка составляет 7.2... После целочисленного
перебора массы добавляемой руды получено, что от 0 до 19 лучшим значением являеся 9, ошибка
достигает значения 5.83 - 5.85(в зависимости от начальных значений). Ошибка на kaggle составляет
5.92.

Нужно подобрать эти неизвестные параметры, поэтому реализована функция от 404 параметра
- 404 параметров - это начальные массы руд в каждом сегменте
- 1 параметр - это добавочная масса

Модель долго обучается, поэтому значения будут получены завтра.

__День 18__
---

До сих пор метод наименьших квадратов не подобрал коэффициенты, поэтому придется
использовать сервисы для дополнительных проверок(таким сервисом выбран google colab), чтобы
подобрать, например, 5 параметров(то есть пусть в 101 сегменте одинаковое начальное кол-во
руд).

Пока производился подбор начальных параметров, сфорулирована новая __гипотеза__:
- Пусть в каждой марке руды содержится какое-то кол-во целевой руды(не факт, что одинаковое)
- При перемешивании - отделение "мусора" от целевой руды.
- Тогда после перемешивание рассматривается не хим реакция веществ, а их сумма(то есть
  целевая руды складывается с предыдущими)
- Остальные принципы и правила получения, как в предыдущей гипотезе.

Реализация этой гипотезы будет после подбора параметров(хотя бы для значения 5). Так как
параметры не подобраны, то на kaggle пока нечего отправлять. Чтобы ускорить подбор
коэффициентов, можно попытаться задать оп ерации через матричные вычисления
:
- операция сдвига - это обнуление вектора(тензора, вообще говоря)
- операция добавления - это суммирование векторов

Проблемными являются операции перемешивания, но:
- локальное перемешивание(в предположении предыдущей гипотезы(той, для которой подбираются сейчас
  коэффициенты)) - присваивание минимума вектору из значений вектора(достаточно быстрая операция)

Глобальное перемешивание трудоемкое, потому что вычисления происходят в цикле, что не эффективно,
поэтому:
- нужно посчитать среднее по вектору - то есть использовать метод `np.mean(axis = ...)`, тогда
  получение константы $a$ будет происходить быстрее, затем можно так же оперировать над
  вектором, путем домножения элементов на описанную выше константу, зависящую от интенсивности.
  
Тогда вычисления будут векторными, а не цикличными, что ускорит подсчет. Если коэффициенты не
будут подобраны до завтра, в 19-ый день реализую ускоренный подсчет.
