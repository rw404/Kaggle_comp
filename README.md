# Kaggle_comp
Дневник своих действий и сохраненные решения с скором

__День 1__:
----
Прочитан [ресурс](https://habr.com/ru/company/ods/blog/426227/) на habr про правильный подход к
решению задачи:
- Нужно заострить внимание на __валидации__, один из возможных ресурсов для машнного обучения на
  выборке: `KFold` из модуля `sklearn`. Разбть выборку на тренировку и тест, затем обучать и
  тестировать. Но как преобразовать потом значения для целевого признака y?
- Коммитить каждый результат со скором, чтобы безболезненно откатиться к стабильному решению
- Просматривать кернелы, но до них еще 2 недели.
- Создать Pipe со всеми методами, который потом модернизировать

Наблюдения:
- Распределения x_*_0 похоже на равномемрное R[0, 3],
- Распределения x_*_1 похоже на нормальное с большим коэффициентм эксцесса,
- Распределения x_*_2 похоже на синус(???), на каждой из x_1_2, x_2_2, x_3_2, ... скачки через
  определенное число элементов
- Распределение целевого признака похоже на нормальное смещенное распределение

Идеи решения:
- Построить нейронную сеть с скрытыми слоями и после кросс-валидации запустить процесс
  обучения(риск: долгая обучение, переобучение)

Решение:
- огромная ошибка, классификация RandomForest не сработала для этой задачи
- огромная ошибка 2, классификация нормального распределения не сработала

__День 2__:
---

Идеи решения:
- Так как задача регрессии, нужно:
    1. Выделить информативные признаки из всех
    2. Через методы регрессии получить их значение
- После этих действий результатом будет скалярное произведение всех 300 массивов-признаков
  размерности 10000(матрица $A \in \R^{10000\times300}$) на полученный в результате регресии вектор 
  $b\in\R^{300}$. Итогом работы программы будет вектор $y = A\times b; y \in \R^{10000}$

Отправки:
- После применения линейной регрессии к данны ошибка достигла 30.
- Проверка метода корреляций -- ошибка 30, нет сильно значимых признаков, так как максимум
  корелляции был 0.11. Возможно есть незначимые признаки, так как минимум -0.02. В ходе проверки
  __не найдены__ признаки, корреляция которых равна 0.

Написана кросс-валидация с параметром n_splits = 10(эксперементально проверенное
значение/советы из литературы), поэтому выборка train разбита на тренировочную и тестовую.


Реализован ансамбль моделей из модуля sklearn. Для каждой модели подсчитывается точность, потом
лучшие варианты своих моделей(например, для моделей KNN и
RandomForestClassifier). 

- В ходе выполнения обучения выявлено, что для KNN оптимальным
значением соседей будет 23.
- В ходе выполнения обучения для модели RandomForest оптимальным значением n_estimators из
  выборки(50, 100, 200) оказалось 100
- Проведено обучение моделей SGDClassifier, LogisticRegression и показаны значения точностей
  этих моделей:
```bash
KNN 0.053
RandomForest 0.072
Logistic 0.046
SGD 0.038
Gaussian 0.051
Ada 0.065
DecTree 0.04

ensemble 0.046(лучший результат 0.057)
```

Лучше всех сработал алгоритм SGD. После проверки выяснилось, что ошибка на тестовых данных
меньше предыдущих, а также в несколько раз меньше ошибок прошлых посылок. После загрузки в kaggle
результат: 6.68..., лучше методов, связанных с корреляцией и регрессиией.

__Подход к задаче с точки зрения теории вероятностей__

Возможно, стоит рассмотреть нормальное распределение с мат.ожиданием 18.3, $\sigma = 6.87$, так
как график распределения похож на распределение y.

Полученный график гистограммы y очень похож на нормальное распределение с указанными
параметрами.

__День 3__
---
Идеи:
- Так как SVG модель лучше справилась с данными, проверить ее на всей выборке(Результат:
  ошибка уменьшилась, но незначительно -- та же ошибка 6)
- Реализовать правильный подсчет корреляции Пирсона для признаков, проверить явные нули, затем
  проверить значимость корреляций через t-критерий Стьюдента для того, чтобы убрать незначимые
  признаки

Из-за невнимательности рассматривалась модель с худшим параметром точности(который
принимался за ошибку), а самая точная модель в реализации кросс-валидации оказалась
Random-forest, поэтому проверена точность модели при различных параметрах n_estimators:
- n_estimators = 10000 => score = 5.36678 
- n_estimators = 1000 => score = 5.36579
- n_estimators = 500 => score = 5.36579

__Очень интересное наблюдение__
Построен массив корреляций признаков с результатом. Для этого массива посчитан t-критерий Стьюдента
и построен доверительный интервал для 95% -- то есть 95%, что случайная величина(t-критерий
Стьюдента корреляции вектора $y$ с признаком $X_i$)
попадет в искомый интервал. Далее для всех t-критериев проводится отбор:
- если t-критерий корреляции $y$ с признаком $X_i$ не входит в доверительный интервал, то
  маловероятно, что оцененная корреляция:
    - будет входить в 95%-ую вероятность(то есть $y$ практически не зависит от этого признака)
    - вовсе не является корреляцией(то есть скрытые признаки выдали сходство, но сам $y$ от
      конкретного признака не зависит)

По результатом отбора получено число признаков, которые прошли отбор и их индексы:
```bash
32
[59, 62, 71, 83, 101, 107, 110, 119, 140, 142, 152, 162, 168, 174, 189, 190, 198, 205, 211, 222,
226, 228, 232, 240, 241, 246, 252, 259, 261, 276, 291, 298]
```

Лишь $\frac{32}{300}$ признаков оказались значимыми.

__Объединение статистических выводов и лучшего регрессора__

Матрица признаков заменена на матрицу значимых признаков, затем по ней проводится оценка
лучшего регрессора из sklearn:
```bash
KNN 0.038608519097921956
RandomForest 0.04170718524912087
Logistic 0.069
SGD -9.574491199245156e+22
Gaussian 0.052
Ada 0.020337533027445565
DecTree 0.036
```

Результатом ансамбля лучших моделей оказался результат: -0.2347772420088041, поэтому используем
для тестирования модель LogisticRegression для тех 32 признаков, которые показались
значимыми. Score = 5.96720.

Если проверить данный алгоритм на модели RandomForest(n_estimators = 1000) для 32-ух данных, то
получится результат 5.43165, который отличается от лучшего на текущий момент такого же
регрессора, но для 300 признаков с score = 5.36579 не очень значительно. То есть сокращение
выборки в 10 раз выявило значимые результаты.

__День 4__
----
Реализована модель метода наиментших квадратов(линейная), применена для 32-х признаков -- score =
5.4

Увеличено доверительный интервал до показателья 99%, теперь значимых 42 признака, для них
применена модель метода наименьших квадратов -- score = 5.22 -- пока что лучший результат

Проверено, что если уменьшить доверительный интервал до 90%, то для того же МНК score увеличится,
поэтому лучше работать с 42-мя признаками.

Для удобства добавлена функция измерения ошибки -- __средняя абсолютная ошибка__

__Итог__: модель лучше работает для 42-х значимых признаков, получаемых в результате подсчета
доверительного интервала для параметра 99%

Нужно попробовать подобрать параметры для лучшей модели регрессии(в ходе оценки ей оказалась модель
RandomForest) -- для понимания значимости параметров ознакомился с 
[материалом](https://dyakonov.org/2016/11/14/%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D0%B9-%D0%BB%D0%B5%D1%81-random-forest/comment-page-1/):
  - Лучшим значением параметра `n_estimators` оказалось значение __5000__
  - Лучшим значением параметра `max_features` оказалось значение (как и написано в материале, так и
    на практике: при больших значениях точность выше, но процесс дольше, поэтому средним выбрано
    пока что значение 5)

  
Проверен метод наименьших квадратов для полинома 3-ей степени, результат лучше: 
- score = 5.04

Выявлены 42 значимых признака, но возможно, что их корреляция не так значима, потому что могут
быть скрытые зависимости от 3-их признаков, поэтому создан алгоритм глубокого обучения для
выявления скрытых признаков. Для глубокого обучения с одним скрытым слоем размерности 128
значение ошибки было 6.39, на тренировочной выборке средняя абсолютная ошибка достигла 6.68, на
новой архитектуре: два скрытых слоя, первый размерности 200, второй - 100 нейронная сеть уже
получает меньшую ошибку. Возможно, результат придется загрузить завтра, так как долгое обучение.

__День 5__
----

- Созданы архитектуры глубокой сети:
    - 1 слой размера 128(score = 6.39)
    - 2 слоя размеров 200 и 100(score = 5.47)
    - 3 слоя размера 200, 100 и 50(score = 7.6..)

Плохой результат произошел из-за недообучения модели или переобучения. Лучшей моделью является двуслойная(200, 100), но нужно ее дообучить.

- Подбор параметров для RandomForestResgressor не осуществлен, так как значительная часть времени была потрачена на разбор работы jupyter notebook на GPU. Иначе модель долго тестировалась, а с GPU возникли проблемы, поэтому нужно доразбраться в алгоритме CUDA и ускорить подбор параметров.

__День 6__
----

Проверка точности метода наименьших квадратов:
- Заранее плохая затея: искать методом нименьших квадратов зависимость в виде полином 12-ой
степени. После 3-х часов поиска ошибка была оочень большой, порядка $10^{12}$
- После сокращения степени полинома до 5-ти, поиск составил примерно минуту, ошибка в смысле `mae`
  составила 4.901214299536031.
- Повышение степени полинома до 6-ти привело к уменьшению ошибки до значения 4.870604365694106
- Повышение степени до значения 7 приводит к ошибке:

```bash
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-53-f031cd6d637d> in <module>
----> 1 cofs, _ = curve_fit(f, Matrix1, y)

~/.local/lib/python3.8/site-packages/scipy/optimize/minpack.py in curve_fit(f, xdata, ydata, p0, sigma, absolute_sigma, check_finite, bounds, method, jac, **kwargs)
    787         cost = np.sum(infodict['fvec'] ** 2)
    788         if ier not in [1, 2, 3, 4]:
--> 789             raise RuntimeError("Optimal parameters not found: " + errmsg)
    790     else:
    791         # Rename maxfev (leastsq) to max_nfev (least_squares), if specified.

RuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 59200.
```
Поэтому наибольшей степенью полинома для оценки методом наименьших квадратов является 6. Для МНК
с полиномом 6-ой степени score = 4.97575 (пока что является лучшим результатом моих отправок).

Теперь пробую провести приближение для всей выборки, а не только для 42-х значимых признаков. При
всей выборке полином 300-ой степени состоит из 1801 переменной, и подбор происходит уже 6 часов,
поэтому принято решение увеличить доверительный интервал до показателя 99.999%, при котором значимыми
показываются 64 признака.

Время выполнения составило примерно 10 минут. score = 4.7777(лучший результат).

Прочитан [ресурс](https://habr.com/ru/company/mailru/blog/513842/) про глубокое обучение в регрессии и другие популярные модели регрессоров.
Замечено, что практически все использованы для проверки или не подходят для данной
задачи(например, модель гребневой регрессии не подходит для данной задачи, так как расчитана на
увеличение кол-ва "значимых" признаков), но нужно протестировать еще регрессоры:
- регрессиия LASSO
- регрессия ElasticNet с параметром $\alpha$ близком к 1, так как при $\alpha = 0$ регрессия
  становится гребневой.

Для применения глубокого обучения использовались доп слои с нелинейными функциями
активации(sigmoid), архитектура состояла из трех слоев: 100, 50, 25, и между ними слои с
функциями активации sigmoid. Обучение долгое, но на каждом этапе сохраняется текущее состояние и
выводится информация об ошибке на выборке, относенной к категории test после применения
кросс-валидайии(то есть будет обнаружено переобучение):
```bash
Средняя абсолютная ошибка: 5.203114032745361 (100 fits)
Средняя абсолютная ошибка: 4.841518402099609 (200 fits)
Средняя абсолютная ошибка: 5.086235046386719 (300 fits)
Средняя абсолютная ошибка: 4.839902877807617 (400 fits)
Средняя абсолютная ошибка: 4.78037166595459 (500 fits)
Средняя абсолютная ошибка: 4.90826940536499 (700 fits)
Средняя абсолютная ошибка: 4.696959018707275 (800 fits)
Средняя абсолютная ошибка: 4.737911224365234 (1000 fits)
Средняя абсолютная ошибка: 4.87761116027832 (1500 fits)
Средняя абсолютная ошибка: 4.773355484008789 (2500 fits)
```

Но модель обучалась на 300 признаках, тем не менее score = 4.84(среди алгоритмов машинного обучения
пока что лучший результат)


Закончен подбор оптимальных параметров для RandomForestRegressor. Параметры:
```bash
{'max_depth': 5, 'max_features': 42, 'n_estimators': 5000, 'n_jobs': -1}
```

Для 64-х признаков результат работы RandomForest составил score = 5.36

__Итоги__:
- Deep learning 3 слоя -- 100, 50, 25 -- 2500 fits -- 300 признаков -- score = 4.84 --
  замечание(взять только значимые признаки, так как выявляются засимости между всеми признаками, в
  том числе и между "шумом")
- RandomForest -- подобраны параметры -- 64 признака -- score = 5.36 -- замечание(max_features =
  число признаков) -- для 300 признаков
- МНК -- зависимость до 6-ой степени -- 64 признака -- score = 4.77 -- замечание(понизить степень и
  сделать для 300 признаков)

__Сделать__:
- LASSO и ElasticNet

__Использован метод МНК кубической зависимости для ВСЕХ признаков и результат оказался score = 3.43 (теперь лучший
результат)__

__День 7__
----
Стали известны имена признаков.

Подробнее рассмотрены типы значений данных:
- в типе операции значения только {0, 1, 2, 3} - 4 операции
- в первом параметре только целые характеристики(от 0 до 100) - предположение: кол-во вещества/
  концентрация(округленная до целого числа)/номер в таблице
- во втором параметре дробные значения от 0 до 3: кол-во дней, деленное на 10(один месяц)

__В ходе проверки гипотез установлено__, что операции с типами 1 и 3 обладают двумя характеристиками(без
-1 среди этих значений), у операций с типом 2 характеристика №1 принимает значение -1, у
операции с типом 0 характеристика №2 принимает значение -1, то есть:
- типы операций 3 и 1 свойсвтенны бинарным операциям(участвуют две характеристики)
- тип операции 2 свойственен унарной операции с 2-ой характеристикой(без участия первой)
- тип операции 0 свойственен унарной операции с 1-ой характеристикой(без участия второй)

Неудачная попытка попробовать нормолизовать 1-ую характеристику, разделив ее на 100, привела к score
= 4.18.

Использование лучшей до знания имен признаков модели(МНК 3-ей степени) к подготовленной к
работе матрице(то есть с замененными минус единицами на ноль) привело к улучшению результата
на 0.00002, теперь score = 3.43255.


__День 8__
---

Гипотеза:
- 1-ая характеристика - концентрация в процентах(например, соли в воде)
- 2-ая характеристика - масса смеси
- типы операций:
    - 0 - операция достижения заданной концентрации
    - 1 - операция добавления смеси заданной массы к имеющемуся раствору
    - 2 - операция добавления воды(нейтральный элемент) к раствору
    - 3 - обратная к 1 операция
- ПРОБЛЕМЫ:
    - в столбце первого признака(операции) наблюдаются как операции добавления, так и операции убавления вещества.
        - Решение: изначально имеется некоторая масса вещества(но как определить?)

Смоделирована модель, основная проблема, что если иметь значения массы соли и массы воды, то начиная с 10-ой(примерно) операции большинство испытаний показывают, что масса соли становится отрицательной. Причины:
- Малая начальная затравка(взять большое значение массы)
- Неверная гипотеза

Чтобы проверить корректность гипотезы рассмотрено грубое приближение(смещенное на мат.ожидание y в положительном направлении, то есть результат += 18.3): в зависимости от операции(1 или 3) прибавлять или убавлять произведение X_i_1*X_i_2/100. 

Тогда для случая, когда 3 - операция '+', 1 - операция '-', ошибка составила 8.94708953, график(зеленая гистограмма - смоделированное значение): 

![](https://psv4.userapi.com/c536132/u140757976/docs/d9/8fda0fd252e2/3_1.png?extra=VKqduhmTLNpg2YSE252exTH46-gqiCFPmbXSBXkR0Kc4utEpAqgUYnn5uRsOJbLHfuMFcPKSGwWpaYYWjphpmo1mnZOooF9H6xwOGuV4mVP_Ek8hU89b6AQviW_Gs0hnC6ajUu4fqq8pED_nl_pZmYc)

Тогда для случая, когда 3 - операция '-', 1 - операция '+', ошибка составила 6.98379433, график(зеленая гистограмма - смоделированное значение): 

![](https://psv4.userapi.com/c536132/u140757976/docs/d15/7ecdca89a725/3-1.png?extra=JXPArC-TUmHG_4pcOh9Hdx-Z3TIV8Rt-yH3j0ma75of5UPr2gCkqTXMz3M088y7rpRByRxcirUiXmH6MmIV3bjBkgjmeUTabTHbtu7RNBHCvyelEDQ5yEeUxgrUjw0hFoDINXpVWFRMdEt-GBrFEE3o)

__День 9__
----

Улучшена предыдущая гипотеза.

__ОПЫТ__: Изучение зависимости растоворимости от температуры

__МАТЕРИАЛ__:
- вода($H_2O$)
- соль(NaCl)

__ДАННЫЕ__:
- 2 характеристики каждой операции:
  - Температура вещества (целое число от 0 до 100 градусов Цельсия)
  - масса(вещественное значение от 0 до 3) в граммах
- 4 операции:
  - 0 - нагрев смеси до заданной температуры
  - 1 - добавление соли заданной температуы заданной массы в раствор
  - 2 - извлечение выпавшего осадка соли
  - 3 - добавление воды заданной температуры заданной массы в раствор
- __РЕЗУЛЬТАТ__ - значение концентрации

__Почему такая гипотеза__: 
- Изменение характеристик, непосредственно оказывающих влияние на растворимость(температура,
  концентрация, включающая массу воды и массу соли)
- Реальность операций: логично их выполнение(нет извлечения заданной концентрации из смеси, как в
  предыдущей гипотезе)
  
__Проблемы__:
- Так как среди операций есть извлечение осадка соли, и эта операция может оказаться первой в
  экспериментах, то нужны изначальные данные: масса воды и масса соли. Как их узнать?
  
__Потенциальное решение__:
- Запустить цикл для отбора, в котором принимаютя различные значения массы воды, массы соли, чтобы
  выполялось два условия:
  - Отсутствие отрицательной массы соли в любой момент эксперимента
  - Минимальность ошибки смоделированной величины и известной

Результат корректности гипотезы запланирован на завтра.
